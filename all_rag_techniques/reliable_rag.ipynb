{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZHxzFohXZpH"
      },
      "source": [
        "### Visual Representation\n",
        "\n",
        "<img src=\"https://github.com/NirDiamant/RAG_Techniques/blob/main/images/reliable_rag.svg?raw=1\" alt=\"Reliable-RAG\" width=\"300\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjPGQQfIXZpJ"
      },
      "source": [
        "# Package Installation and Imports\n",
        "\n",
        "The cell below installs all necessary packages required to run this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sIJHX4NXZpJ"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install langchain langchain-community python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "km8fDiniXZpK"
      },
      "outputs": [],
      "source": [
        "### LLMs\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from '.env' file\n",
        "load_dotenv()\n",
        "\n",
        "os.environ['GROQ_API_KEY'] = os.getenv('GROQ_API_KEY') # For LLM -- llama-3.1-8b (small) & mixtral-8x7b-32768 (large)\n",
        "os.environ['COHERE_API_KEY'] = os.getenv('COHERE_API_KEY') # For embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TaWXkdDXZpK"
      },
      "source": [
        "### Create Vectorstore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjHtmqINXZpK"
      },
      "outputs": [],
      "source": [
        "### Build Index\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter # Import a tool to split text into smaller, manageable chunks\n",
        "from langchain_community.document_loaders import WebBaseLoader # Import a tool to load documents from web pages\n",
        "from langchain_community.vectorstores import Chroma # Import Chroma, a type of database optimized for storing numerical representations (embeddings) of text\n",
        "from langchain_cohere import CohereEmbeddings # Import CohereEmbeddings to convert text into numerical representations (embeddings)\n",
        "\n",
        "# Set embeddings\n",
        "embedding_model = CohereEmbeddings(model=\"embed-english-v3.0\") # Choose Cohere's embedding model to create numerical representations of text\n",
        "\n",
        "# Docs to index\n",
        "urls = [\n",
        "    \"https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/?ref=dl-staging-website.ghost.io\", # List of web page addresses (URLs) to get content from\n",
        "    \"https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-2-reflection/?ref=dl-staging-website.ghost.io\",\n",
        "    \"https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-3-tool-use/?ref=dl-staging-website.ghost.io\",\n",
        "    \"https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-4-planning/?ref=dl-staging-website.ghost.io\",\n",
        "    \"https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-5-multi-agent-collaboration/?ref=dl-staging-website.ghost.io\"\n",
        "]\n",
        "\n",
        "# Load\n",
        "docs = [WebBaseLoader(url).load() for url in urls] # Go to each URL and load its content as documents\n",
        "docs_list = [item for sublist in docs for item in sublist] # Flatten the list of documents into a single list\n",
        "\n",
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder( # Set up a text splitter to break down large documents\n",
        "    chunk_size=500, chunk_overlap=0 # Each chunk will be up to 500 characters, with no overlap between them\n",
        ")\n",
        "doc_splits = text_splitter.split_documents(docs_list) # Split the loaded documents into smaller chunks\n",
        "\n",
        "# Add to vectorstore\n",
        "vectorstore = Chroma.from_documents( # Create a Chroma vector store (a database for embeddings)\n",
        "    documents=doc_splits, # Add the document chunks to the vector store\n",
        "    collection_name=\"rag\", # Give a name to this collection of documents\n",
        "    embedding=embedding_model, # Use the Cohere embedding model to convert chunks into embeddings\n",
        ")\n",
        "\n",
        "retriever = vectorstore.as_retriever( # Create a retriever that can fetch relevant documents from the vector store\n",
        "                search_type=\"similarity\", # Search for documents that are most similar to a given query\n",
        "                search_kwargs={'k': 4}, # Retrieve the top 4 most similar documents\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYgUniiuXZpK"
      },
      "source": [
        "### Question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aI8NSkHwXZpL"
      },
      "outputs": [],
      "source": [
        "question = \"what are the differnt kind of agentic design patterns?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsZ_GWCqXZpL"
      },
      "source": [
        "### Retrieve docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dhSlnacXZpL"
      },
      "outputs": [],
      "source": [
        "docs = retriever.invoke(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auH_3TxBXZpL"
      },
      "source": [
        "### Check what our doc looklike"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HR7X5gMOXZpL",
        "outputId": "b431d3be-7f28-4c27-856a-cb10395f35b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Title: Agentic Design Patterns Part 5, Multi-Agent Collaboration\n",
            "\n",
            "Source: https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-5-multi-agent-collaboration/?ref=dl-staging-website.ghost.io\n",
            "\n",
            "Content: mature patterns of Reflection and Tool Use are more reliable. I hope you enjoy playing with these agentic design patterns and that they produce amazing results for you! If you're interested in learning more, I recommend: ‚ÄúCommunicative Agents for Software Development,‚Äù Qian et al. (2023) (the ChatDev paper)‚ÄúAutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation,‚Äù Wu et al. (2023) ‚ÄúMetaGPT: Meta Programming for a Multi-Agent Collaborative Framework,‚Äù Hong et al. (2023)Keep learning!AndrewRead \"Agentic Design Patterns Part 1: Four AI agent strategies that improve GPT-4 and GPT-3.5 performance\"Read \"Agentic Design Patterns Part 2: Reflection\" Read \"Agentic Design Patterns Part 3: Tool Use\"Read \"Agentic Design Patterns Part 4: Planning\" ShareSubscribe to The BatchStay updated with weekly AI News and Insights delivered to your inboxCoursesThe BatchCommunityCareersAbout\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"Title: {docs[0].metadata['title']}\\n\\nSource: {docs[0].metadata['source']}\\n\\nContent: {docs[0].page_content}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoBfhfB6XZpM"
      },
      "source": [
        "### Check document relevancy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fd4XdPMVXZpM"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate # Import a tool to create structured chat prompts\n",
        "from pydantic import BaseModel, Field # Import tools to define data structures with validation\n",
        "from langchain_groq import ChatGroq # Import the Groq chat model for generating responses\n",
        "\n",
        "# Data model\n",
        "class GradeDocuments(BaseModel): # Define a structure for how we expect the grading result to look\n",
        "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
        "\n",
        "    binary_score: str = Field(\n",
        "        description=\"Documents are relevant to the question, 'yes' or 'no'\" # This field will store 'yes' if a document is relevant, or 'no' if it's not\n",
        "    )\n",
        "\n",
        "\n",
        "# LLM with function call\n",
        "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0) # Initialize a large language model (LLM) from Groq, setting its creativity to a minimum (temperature=0)\n",
        "structured_llm_grader = llm.with_structured_output(GradeDocuments) # Configure the LLM to output its answers in the 'GradeDocuments' structure we defined\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
        "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
        "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
        "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\" # Define the instructions for the LLM on how to grade document relevance\n",
        "grade_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system), # Add the system instructions to the prompt\n",
        "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"), # Define the user's part of the prompt, including placeholders for the document and question\n",
        "    ]\n",
        ")\n",
        "\n",
        "retrieval_grader = grade_prompt | structured_llm_grader # Combine the prompt and the structured LLM to create our document grading chain. | is langchain's pipe operator.\n",
        "#Here, the output of grade_prompt is fed as input into structured_llm_grader, creating a sequential chain of operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEg7BspcXZpM"
      },
      "source": [
        "### Filter out the non-relevant docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XzhCC7xXZpM",
        "outputId": "eddcfc0b-e092-465f-ebc9-d71dc8242f30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mature patterns of Reflection and Tool Use are more reliable. I hope you enjoy playing with these agentic design patterns and that they produce amazing results for you! If you're interested in learning more, I recommend: ‚ÄúCommunicative Agents for Software Development,‚Äù Qian et al. (2023) (the ChatDev paper)‚ÄúAutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation,‚Äù Wu et al. (2023) ‚ÄúMetaGPT: Meta Programming for a Multi-Agent Collaborative Framework,‚Äù Hong et al. (2023)Keep learning!AndrewRead \"Agentic Design Patterns Part 1: Four AI agent strategies that improve GPT-4 and GPT-3.5 performance\"Read \"Agentic Design Patterns Part 2: Reflection\" Read \"Agentic Design Patterns Part 3: Tool Use\"Read \"Agentic Design Patterns Part 4: Planning\" ShareSubscribe to The BatchStay updated with weekly AI News and Insights delivered to your inboxCoursesThe BatchCommunityCareersAbout \n",
            " --------------------------------------------------\n",
            "binary_score='yes' \n",
            "\n",
            "I recommend: ‚ÄúGorilla: Large Language Model Connected with Massive APIs,‚Äù Patil et al. (2023)‚ÄúMM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action,‚Äù Yang et al. (2023)‚ÄúEfficient Tool Use with Chain-of-Abstraction Reasoning,‚Äù Gao et al. (2024)   Both Tool Use and Reflection, which I described in last week‚Äôs letter, are design patterns that I can get to work fairly reliably on my applications ‚Äî both are capabilities well worth learning about. In future letters, I‚Äôll describe the Planning and Multi-agent collaboration design patterns. They allow AI agents to do much more but are less mature, less predictable ‚Äî albeit very exciting ‚Äî technologies. Keep learning!AndrewRead \"Agentic Design Patterns Part 1: Four AI agent strategies that improve GPT-4 and GPT-3.5 performance\"Read \"Agentic Design Patterns Part 2: Reflection\"Read \"Agentic Design Patterns Part 4: Planning\"Read \"Agentic Design Patterns Part 5: Multi-Agent Collaboration\"ShareSubscribe to The BatchStay updated with weekly AI News and Insights delivered to your inboxCoursesThe BatchCommunityCareersAbout \n",
            " --------------------------------------------------\n",
            "binary_score='yes' \n",
            "\n",
            "67.0%. However, the improvement from GPT-3.5 to GPT-4 is dwarfed by incorporating an iterative agent workflow. Indeed, wrapped in an agent loop, GPT-3.5 achieves up to 95.1%. Open source agent tools and the academic literature on agents are proliferating, making this an exciting time but also a confusing one. To help put this work into perspective, Iâ€™d like to share a framework for categorizing design patterns for building agents. My team AI Fund is successfully using these patterns in many applications, and I hope you find them useful.Reflection: The LLM examines its own work to come up with ways to improve it. Tool Use: The LLM is given tools such as web search, code execution, or any other function to help it gather information, take action, or process data.Planning: The LLM comes up with, and executes, a multistep plan to achieve a goal (for example, writing an outline for an essay, then doing online research, then writing a draft, and so on).Multi-agent collaboration: More than one AI agent work together, splitting up tasks and discussing and debating ideas, to come up with better solutions than a single agent would.Next week, Iâ€™ll elaborate on these design patterns and offer suggested readings for each.Keep learning!AndrewRead \"Agentic Design Patterns Part 2: Reflection\"Read \"Agentic Design Patterns Part 3, Tool Use\"Read \"Agentic Design Patterns Part 4: Planning\"Read \"Agentic Design Patterns Part 5: Multi-Agent Collaboration\"ShareSubscribe to The BatchStay updated with weekly AI News and Insights delivered to your inboxCoursesThe BatchCommunityCareersAbout \n",
            " --------------------------------------------------\n",
            "binary_score='yes' \n",
            "\n",
            "Agentic Design Patterns Part 4: Planningüåü New Course! Enroll in Large Multimodal Model Prompting with GeminiExplore CoursesAI NewsletterThe BatchAndrew's LetterData PointsML ResearchBlogCommunityForumEventsAmbassadorsAmbassador SpotlightResourcesCompanyAboutCareersContactStart LearningWeekly IssuesAndrew's LettersData PointsML ResearchBusinessScienceAI & SocietyCultureHardwareAI CareersAboutSubscribeThe BatchLettersArticleAgentic Design Patterns Part 4, Planning Large language models can drive powerful agents to execute complex tasks if you ask them to plan the steps before they act.LettersTechnical InsightsPublishedApr 10, 2024Reading time3 min readShareDear friends,Planning is a key agentic AI design pattern in which we use a large language model (LLM) to autonomously decide on what sequence of steps to execute to accomplish a larger task. For example, if we ask an agent to do online research on a given topic, we might use an LLM to break down the objective into smaller subtasks, such as researching specific subtopics, synthesizing findings, and compiling a report. Many people had a ‚ÄúChatGPT moment‚Äù shortly after ChatGPT was released, when they played with it and were surprised that it significantly exceeded their expectation of what AI can do. If you have not yet had a similar ‚ÄúAI Agentic moment,‚Äù I hope you will soon. I had one several months ago, when I presented a live demo of a research agent I had implemented that had access to various online search tools. I had tested this agent multiple times privately, during which it consistently used a web search tool to gather information and wrote up a summary. During the live demo, though, the web search API unexpectedly returned with a rate limiting error. I thought my demo was about to fail publicly, and I dreaded what was to come next. To my surprise, the agent pivoted deftly to a Wikipedia search tool ‚Äî which I had forgotten I‚Äôd given it ‚Äî and completed the task using Wikipedia instead of web search. This was an AI Agentic moment of surprise for me. I think many people who haven‚Äôt experienced such a moment yet will do so in the coming months. It‚Äôs \n",
            " --------------------------------------------------\n",
            "binary_score='yes' \n",
            "\n"
          ]
        }
      ],
      "source": [
        "docs_to_use = []\n",
        "for doc in docs:\n",
        "    print(doc.page_content, '\\n', '-'*50)\n",
        "    res = retrieval_grader.invoke({\"question\": question, \"document\": doc.page_content})\n",
        "    print(res,'\\n')\n",
        "    if res.binary_score == 'yes':\n",
        "        docs_to_use.append(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwORY6pmXZpM"
      },
      "source": [
        "### Generate Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBo-clbrXZpM",
        "outputId": "e318e02f-bd7f-4b33-cc55-b93dc85215e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "According to the retrieved documents, there are four main agentic design patterns:\n",
            "\n",
            "1. **Reflection**: The LLM examines its own work to come up with ways to improve it.\n",
            "2. **Tool Use**: The LLM is given tools such as web search, code execution, or any other function to help it gather information, take action, or process data.\n",
            "3. **Planning**: The LLM comes up with, and executes, a multistep plan to achieve a goal.\n",
            "4. **Multi-agent collaboration**: More than one AI agent work together, splitting up tasks and discussing and debating ideas, to come up with better solutions than a single agent would.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser # Import a tool to convert the LLM's output into a simple string\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are an assistant for question-answering tasks. Answer the question based upon your knowledge.\n",
        "Use three-to-five sentences maximum and keep the answer concise.\"\"\" # Define the instructions for the AI assistant\n",
        "prompt = ChatPromptTemplate.from_messages( # Create a chat prompt using the system instructions and a human query\n",
        "    [\n",
        "        (\"system\", system), # Add the system instructions to the prompt\n",
        "        (\"human\", \"Retrieved documents: \\n\\n <docs>{documents}</docs> \\n\\n User question: <question>{question}</question>\"), # Define the user's part of the prompt, including placeholders for documents and question\n",
        "    ]\n",
        ")\n",
        "\n",
        "# LLM\n",
        "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0) # Initialize a large language model (LLM) from Groq, setting its creativity to a minimum\n",
        "\n",
        "# Post-processing\n",
        "def format_docs(docs): # Define a function to format the retrieved documents nicely\n",
        "    return \"\\n\".join(f\"<doc{i+1}>:\\nTitle:{doc.metadata['title']}\\nSource:{doc.metadata['source']}\\nContent:{doc.page_content}\\n</doc{i+1}>\\n\" for i, doc in enumerate(docs)) # Loop through documents and format each with its title, source, and content\n",
        "\n",
        "# Chain\n",
        "rag_chain = prompt | llm | StrOutputParser() # Create a sequence: prompt -> LLM -> string parser to generate the answer\n",
        "\n",
        "# Run\n",
        "generation = rag_chain.invoke({\"documents\":format_docs(docs_to_use), \"question\": question}) # Run the chain with formatted documents and the user's question to get the answer\n",
        "print(generation) # Display the generated answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xYQxUeWXZpM"
      },
      "source": [
        "### Check for Hallucinations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAm7BiTpXZpM",
        "outputId": "17fc1000-2063-4f80-d125-976de814ce62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "binary_score='yes'\n"
          ]
        }
      ],
      "source": [
        "# Data model\n",
        "class GradeHallucinations(BaseModel):\n",
        "    \"\"\"Binary score for hallucination present in 'generation' answer.\"\"\"\n",
        "\n",
        "    binary_score: str = Field(\n",
        "        ...,\n",
        "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "# LLM with function call\n",
        "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0)\n",
        "structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n\n",
        "    Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
        "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"Set of facts: \\n\\n <facts>{documents}</facts> \\n\\n LLM generation: <generation>{generation}</generation>\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "hallucination_grader = hallucination_prompt | structured_llm_grader\n",
        "\n",
        "response = hallucination_grader.invoke({\"documents\": format_docs(docs_to_use), \"generation\": generation})\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj7w2LR8XZpN"
      },
      "source": [
        "### Highlight used docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaeDA7R5XZpN"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# Data model\n",
        "class HighlightDocuments(BaseModel):\n",
        "    \"\"\"Return the specific part of a document used for answering the question.\"\"\"\n",
        "\n",
        "    id: List[str] = Field(\n",
        "        ...,\n",
        "        description=\"List of id of docs used to answers the question\"\n",
        "    )\n",
        "\n",
        "    title: List[str] = Field(\n",
        "        ...,\n",
        "        description=\"List of titles used to answers the question\"\n",
        "    )\n",
        "\n",
        "    source: List[str] = Field(\n",
        "        ...,\n",
        "        description=\"List of sources used to answers the question\"\n",
        "    )\n",
        "\n",
        "    segment: List[str] = Field(\n",
        "        ...,\n",
        "        description=\"List of direct segements from used documents that answers the question\"\n",
        "    )\n",
        "\n",
        "# LLM\n",
        "llm = ChatGroq(model=\"mixtral-8x7b-32768\", temperature=0)\n",
        "\n",
        "# parser\n",
        "parser = PydanticOutputParser(pydantic_object=HighlightDocuments)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are an advanced assistant for document search and retrieval. You are provided with the following:\n",
        "1. A question.\n",
        "2. A generated answer based on the question.\n",
        "3. A set of documents that were referenced in generating the answer.\n",
        "\n",
        "Your task is to identify and extract the exact inline segments from the provided documents that directly correspond to the content used to\n",
        "generate the given answer. The extracted segments must be verbatim snippets from the documents, ensuring a word-for-word match with the text\n",
        "in the provided documents.\n",
        "\n",
        "Ensure that:\n",
        "- (Important) Each segment is an exact match to a part of the document and is fully contained within the document text.\n",
        "- The relevance of each segment to the generated answer is clear and directly supports the answer provided.\n",
        "- (Important) If you didn't used the specific document don't mention it.\n",
        "\n",
        "Used documents: <docs>{documents}</docs> \\n\\n User question: <question>{question}</question> \\n\\n Generated answer: <answer>{generation}</answer>\n",
        "\n",
        "<format_instruction>\n",
        "{format_instructions}\n",
        "</format_instruction>\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template= system,\n",
        "    input_variables=[\"documents\", \"question\", \"generation\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "# Chain\n",
        "doc_lookup = prompt | llm | parser\n",
        "\n",
        "# Run\n",
        "lookup_response = doc_lookup.invoke({\"documents\":format_docs(docs_to_use), \"question\": question, \"generation\": generation})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjULYr70XZpN",
        "outputId": "4310f7db-5854-4bb4-c73e-d3a0dafb4d64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ID: doc3\n",
            "Title: Four AI Agent Strategies That Improve GPT-4 and GPT-3.5 Performance\n",
            "Source: https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/?ref=dl-staging-website.ghost.io\n",
            "Text Segment: Reflection: The LLM examines its own work to come up with ways to improve it.\\nTool Use: The LLM is given tools such as web search, code execution, or any other function to help it gather information, take action, or process data.\\nPlanning: The LLM comes up with, and executes, a multistep plan to achieve a goal (for example, writing an outline for an essay, then doing online research, then writing a draft, and so on).\\nMulti-agent collaboration: More than one AI agent work together, splitting up tasks and discussing and debating ideas, to come up with better solutions than a single agent would.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for id, title, source, segment in zip(lookup_response.id, lookup_response.title, lookup_response.source, lookup_response.segment):\n",
        "    print(f\"ID: {id}\\nTitle: {title}\\nSource: {source}\\nText Segment: {segment}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HPEmp2UXZpN"
      },
      "source": [
        "### Text segment in the source\n",
        "\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93xuLsrhXZpN"
      },
      "source": [
        "![](https://europe-west1-rag-techniques-views-tracker.cloudfunctions.net/rag-techniques-tracker?notebook=all-rag-techniques--reliable-rag)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}